{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This is the beginning of Cloaky-LM"
      ],
      "metadata": {
        "id": "hZHrrpYNHcHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the libraries"
      ],
      "metadata": {
        "id": "PP6xbtEwHjHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. torch: This is PyTorch, the fundamental deep-learning framework we will use. Think of it as the engine and raw materials (like steel and circuits) for our model.\n",
        "\n",
        "2. transformers: From Hugging Face, this is the most important library for our project. It provides pre-built architectures (like the Transformer) and high-level tools, including a Trainer class that will manage our training loop for us. It's our master blueprint and toolbox.\n",
        "\n",
        "3. datasets: Also from Hugging Face, this library makes it incredibly simple to download, load, and process the vast amounts of text data our model needs to learn from.\n",
        "\n",
        "4. tokenizers: An efficient library for the crucial step of converting our text into numbers that the model can understand.\n",
        "\n",
        "5. accelerate: A helper library that works with transformers to automatically optimize our training code to run efficiently on whatever hardware we have (like the T4 GPU in Colab).\n"
      ],
      "metadata": {
        "id": "gUJvE5hZIKyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tokenizers torch accelerate\n",
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "mxBMX4CpHb9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Download and load the wikitext-2-raw-v1 configuration of the WikiText dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "aVc91VeaIfI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying some examples of the training data\n",
        "print(dataset[\"train\"][9]['text'])"
      ],
      "metadata": {
        "id": "Qr4LnJy3J6jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenising the dataset with eos"
      ],
      "metadata": {
        "id": "TP0D-MwEfdL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer of the 'gpt2' model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\"\"\"\n",
        "The GPT-2 model was trained without a padding token.\n",
        "Generally a End of sentence token is used as a padding Token...\n",
        "\n",
        "I am going to use the eos token as the padding token\n",
        "\"\"\"\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  \"\"\"it takes the text and converts it to 'input_ids'.\"\"\"\n",
        "  return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "C1ImSaMeM3LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   We loaded a tokenizer that already knows a vocabulary of about 50,000 English tokens.\n",
        "*   used .map() to apply this function across all splits (train, validation, test) of our dataset\n",
        "\n",
        "*   remove_columns=[\"text\"] because once we have the input_ids, we no longer need the original raw text\n"
      ],
      "metadata": {
        "id": "XX_TftJBcFzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 20 token IDs for the 10th example.\n",
        "print(tokenized_datasets[\"train\"][9]['input_ids'][:20])\n"
      ],
      "metadata": {
        "id": "QTAcCTx-cR-H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}