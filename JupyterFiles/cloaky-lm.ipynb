{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# This is the beginning of Cloaky-LM"
      ],
      "metadata": {
        "id": "hZHrrpYNHcHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the libraries"
      ],
      "metadata": {
        "id": "PP6xbtEwHjHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. torch: This is PyTorch, the fundamental deep-learning framework we will use. Think of it as the engine and raw materials (like steel and circuits) for our model.\n",
        "\n",
        "2. transformers: From Hugging Face, this is the most important library for our project. It provides pre-built architectures (like the Transformer) and high-level tools, including a Trainer class that will manage our training loop for us. It's our master blueprint and toolbox.\n",
        "\n",
        "3. datasets: Also from Hugging Face, this library makes it incredibly simple to download, load, and process the vast amounts of text data our model needs to learn from.\n",
        "\n",
        "4. tokenizers: An efficient library for the crucial step of converting our text into numbers that the model can understand.\n",
        "\n",
        "5. accelerate: A helper library that works with transformers to automatically optimize our training code to run efficiently on whatever hardware we have (like the T4 GPU in Colab).\n"
      ],
      "metadata": {
        "id": "gUJvE5hZIKyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tokenizers torch accelerate\n",
        "!pip install --upgrade datasets"
      ],
      "metadata": {
        "id": "mxBMX4CpHb9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Download and load the wikitext-2-raw-v1 configuration of the WikiText dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "aVc91VeaIfI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying some examples of the training data\n",
        "print(dataset[\"train\"][9]['text'])"
      ],
      "metadata": {
        "id": "Qr4LnJy3J6jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenising the dataset with eos"
      ],
      "metadata": {
        "id": "TP0D-MwEfdL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the tokenizer of the 'gpt2' model.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "\"\"\"\n",
        "The GPT-2 model was trained without a padding token.\n",
        "Generally a End of sentence token is used as a padding Token...\n",
        "\n",
        "I am going to use the eos token as the padding token\n",
        "\"\"\"\n",
        "tokenizer.pad_token=tokenizer.eos_token\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  \"\"\"it takes the text and converts it to 'input_ids'.\"\"\"\n",
        "  return tokenizer(examples[\"text\"], truncation=True, max_length=128)\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "C1ImSaMeM3LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   We loaded a tokenizer that already knows a vocabulary of about 50,000 English tokens.\n",
        "*   used .map() to apply this function across all splits (train, validation, test) of our dataset\n",
        "\n",
        "*   remove_columns=[\"text\"] because once we have the input_ids, we no longer need the original raw text\n"
      ],
      "metadata": {
        "id": "XX_TftJBcFzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first 20 token IDs for the 10th example.\n",
        "print(tokenized_datasets[\"train\"][9]['input_ids'][:20])\n"
      ],
      "metadata": {
        "id": "QTAcCTx-cR-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Blueprint"
      ],
      "metadata": {
        "id": "Ml0iCz5oC60H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, AutoModelForCausalLM\n",
        "\n",
        "vocab_size = tokenizer.vocab_size\n",
        "\n",
        "config = AutoConfig.from_pretrained(\n",
        "    \"gpt2\",  # Use the GPT-2 architecture as a template.\n",
        "    vocab_size=vocab_size,\n",
        "    n_positions=128,      # Maximum sequence length the model can handle (matches our tokenizer).\n",
        "    n_embd=256,           # The dimensionality of the token embeddings (the \"richness\" of the word vectors).\n",
        "    n_layer=4,            # The number of Transformer layers (the \"depth\" of the model).\n",
        "    n_head=4,             # The number of attention heads in each layer.\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "id": "Qu_dZLuDC6hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_params=model.num_parameters()\n",
        "print(f\"model has {num_params:,} parameters.\")"
      ],
      "metadata": {
        "id": "VfLEZrSODltk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "aZO_LmO0FbOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "# Check if  GPU is available?\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device= torch.device(\"cuda\")\n",
        "  print(f\"Training on {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "  device= torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "EcWzRML9E8ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "# this collator will automatically create labels for our language modeling task\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False # We want casual (next-token) prediction not masked language modeling.\n",
        ")"
      ],
      "metadata": {
        "id": "QQzNRLF2Flvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./cloakylm-1\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,                # You can increase for better results\n",
        "    per_device_train_batch_size=32,    # Adjust if you get out-of-memory errors\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    report_to=\"none\",                  # Disable wandb/tensorboard for simplicity\n",
        "    fp16=True,                         # Use mixed precision for faster training on GPU\n",
        ")"
      ],
      "metadata": {
        "id": "RJqHGvIEF45W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initialize the trainer"
      ],
      "metadata": {
        "id": "dNvMHEvhGeN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets[\"train\"],\n",
        "    eval_dataset = tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "lcBuYdSbGcsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "iigYkljNGyOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/cloakylm-1\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/cloakylm-1\")\n"
      ],
      "metadata": {
        "id": "bdPhdX1gIkBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/cloakylm-1\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "prompt = \"who are you\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "# Try generating text on CPU\n",
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_length=80,\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    do_sample=True,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "id": "opDlCYygJEgo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}